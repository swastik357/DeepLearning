{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: FFNs\n",
    "### LECTURE: CodeChallenge: MNIST and breadth vs. depth\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4x/pds2gxg17h57rrp30d736s1w0000gn/T/ipykernel_46852/814135284.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  display.set_matplotlib_formats('svg')\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "outputs": [],
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# extract labels (number IDs) and remove from data\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# normalize the data to a range of [0 1]\n",
    "dataNorm = data / np.max(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhLQ2YSvpiGj"
   },
   "source": [
    "# Create train/test groups using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Y_tZ1ymVp0Sf"
   },
   "outputs": [],
   "source": [
    "# Step 1: convert to tensor\n",
    "dataT   = torch.tensor( dataNorm ).float()\n",
    "labelsT = torch.tensor( labels ).long()\n",
    "\n",
    "# Step 2: use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
    "\n",
    "\n",
    "# Step 3: convert into PyTorch Datasets\n",
    "train_data = torch.utils.data.TensorDataset(train_data,train_labels)\n",
    "test_data  = torch.utils.data.TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Step 4: translate into dataloader objects\n",
    "batchsize    = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK8Opkhgp0bO"
   },
   "source": [
    "# Create the DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JK3OO3tAtZkA"
   },
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheMNISTNet(nUnits,nLayers):\n",
    "\n",
    "  class mnistNet(nn.Module):\n",
    "    def __init__(self,nUnits,nLayers):\n",
    "      super().__init__()\n",
    "\n",
    "      # create dictionary to store the layers\n",
    "      self.layers = nn.ModuleDict()\n",
    "      self.nLayers = nLayers\n",
    "\n",
    "      ### input layer\n",
    "      self.layers['input'] = nn.Linear(784,nUnits)\n",
    "      \n",
    "      ### hidden layers\n",
    "      for i in range(nLayers):\n",
    "        self.layers[f'hidden{i}'] = nn.Linear(nUnits,nUnits)\n",
    "\n",
    "      ### output layer\n",
    "      self.layers['output'] = nn.Linear(nUnits,10)\n",
    "    \n",
    "\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      # input layer (note: the code in the video omits the relu after this layer)\n",
    "      x = F.relu( self.layers['input'](x) )\n",
    "\n",
    "      # hidden layers\n",
    "      for i in range(self.nLayers):\n",
    "        x = F.relu( self.layers[f'hidden{i}'](x) )\n",
    "      \n",
    "      # return output layer\n",
    "      x = self.layers['output'](x)\n",
    "      return x\n",
    "  \n",
    "  # create the model instance\n",
    "  net = mnistNet(nUnits,nLayers)\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(net.parameters(),lr=.01,weight_decay=0.001)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "y6icEJcXp0el"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(mnistNet(\n",
       "   (layers): ModuleDict(\n",
       "     (input): Linear(in_features=784, out_features=12, bias=True)\n",
       "     (hidden0): Linear(in_features=12, out_features=12, bias=True)\n",
       "     (hidden1): Linear(in_features=12, out_features=12, bias=True)\n",
       "     (hidden2): Linear(in_features=12, out_features=12, bias=True)\n",
       "     (hidden3): Linear(in_features=12, out_features=12, bias=True)\n",
       "     (output): Linear(in_features=12, out_features=10, bias=True)\n",
       "   )\n",
       " ),\n",
       " CrossEntropyLoss(),\n",
       " SGD (\n",
       " Parameter Group 0\n",
       "     dampening: 0\n",
       "     differentiable: False\n",
       "     foreach: None\n",
       "     lr: 0.01\n",
       "     maximize: False\n",
       "     momentum: 0\n",
       "     nesterov: False\n",
       "     weight_decay: 0.001\n",
       " ))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate an instance of the model and confirm that it returns the expected network.\n",
    "nUnitsPerLayer = 12\n",
    "nLayers = 4\n",
    "net = createTheMNISTNet(nUnitsPerLayer,nLayers)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvfGQIRGp0ht"
   },
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IblJo1NCp0kl"
   },
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "def function2trainTheModel(nUnits,nLayers):\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 60\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createTheMNISTNet(nUnits,nLayers)\n",
    "\n",
    "  # initialize losses\n",
    "  losses    = torch.zeros(numepochs)\n",
    "  trainAcc  = []\n",
    "  testAcc   = []\n",
    "\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "\n",
    "      # compute accuracy\n",
    "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
    "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
    "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
    "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "    # test accuracy\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = net(X)\n",
    "      \n",
    "    # compare the following really long line of code to the training accuracy lines\n",
    "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses,net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpGm9xdQ27Ob"
   },
   "source": [
    "# Run the model and show the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "l9pCC1R2p0nu"
   },
   "outputs": [],
   "source": [
    "# initialize output matrices\n",
    "accuracies  = np.zeros((2,2))\n",
    "timeTaken = np.zeros(2)\n",
    "\n",
    "# start the experiment!\n",
    "start = time.time()\n",
    "trainAcc,testAcc,losses,net = function2trainTheModel(250,3)\n",
    "end = time.time()\n",
    "accuracies[0,0] = np.mean(trainAcc[-5:])\n",
    "accuracies[0,1] = np.mean(testAcc[-5:])\n",
    "timeTaken[0] = end - start\n",
    "   \n",
    "start = time.time()\n",
    "trainAcc,testAcc,losses,net = function2trainTheModel(50,1)\n",
    "end = time.time()\n",
    "accuracies[1,0] = np.mean(trainAcc[-5:])\n",
    "accuracies[1,1] = np.mean(testAcc[-5:])\n",
    "timeTaken[1] = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for best model is 99.94, Test accuracy for best model is 96.27 in time 49.73\n",
      "Train accuracy for worst model is 97.67, Test accuracy for worst model is 95.44 in time 18.2\n"
     ]
    }
   ],
   "source": [
    "print(f'Train accuracy for best model is {np.round(accuracies[0,0],2)}, Test accuracy for best model is {np.round(accuracies[0,1],2)} in time {np.round(timeTaken[0],2)}')\n",
    "\n",
    "print(f'Train accuracy for worst model is {np.round(accuracies[1,0],2)}, Test accuracy for worst model is {np.round(accuracies[1,1],2)} in time {np.round(timeTaken[1],2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh28k_l29urR"
   },
   "source": [
    "# Optional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ib3uQtfv9wE2"
   },
   "outputs": [],
   "source": [
    "# 1) Based on the results above, pick the best and the worst model architectures.\n",
    "#    Then, run only those two models again using regularization (you can pick which\n",
    "#    regularization method to use). Does this help the bad model and/or hurt the good model?\n",
    "# \n",
    "# 2) Time how long each model takes to train (from the full experiment, not just the two models \n",
    "#    for exploration #1). Obviously, both factors (depth and breadth) affect training time, but\n",
    "#    which factor seems to have a bigger influence on model training time?\n",
    "# MY COMMENTS: timeTaken (units X layers) = \n",
    "# [[12.46976995 11.95402288 12.49326897]\n",
    "#  [11.69497967 13.89886498 15.71660185]\n",
    "#  [13.26674366 15.54981709 16.82481813]\n",
    "#  [16.39725208 18.72031593 21.02098823]\n",
    "#  [18.54376602 21.37667799 23.61907697]] Width, i.e., units seem to be more important"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCSKHL3MIR1a7dqScdhWus",
   "provenance": [
    {
     "file_id": "1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM",
     "timestamp": 1617878430303
    },
    {
     "file_id": "1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW",
     "timestamp": 1617803880910
    },
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
